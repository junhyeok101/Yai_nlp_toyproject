# 0. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
!pip install datasets transformers scikit-learn torch sentencepiece

# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments
)
import torch
from sklearn.metrics import accuracy_score



# 2. KoBERT í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("skt/kobert-base-v1")
print("âœ… KoBERT í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")


# 3. CSV íŒŒì¼ â†’ Hugging Face Dataset ë¡œë“œ
dataset = load_dataset("csv", data_files={"train": "review.csv"})
print("âœ… CSV ë¡œë“œ ì™„ë£Œ")

# ë°ì´í„° ì»¬ëŸ¼ëª… í™•ì¸
print("ğŸ“Š ë°ì´í„° ì»¬ëŸ¼:", dataset["train"].column_names)
# ì¶œë ¥ ì˜ˆ) ["review", "label"]

# 4. í† í¬ë‚˜ì´ì € ì ìš© í•¨ìˆ˜
################################


def tokenize_function(examples):
    return tokenizer(examples["review"], padding="max_length", truncation=True, max_length=128, return_token_type_ids=False )


# 5. ì „ì²´ ë°ì´í„°ì…‹ì— í† í°í™” ì ìš©

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# ì›ë³¸ ì»¬ëŸ¼ "review" ì‚­ì œ (ì´ë¯¸ í† í°ìœ¼ë¡œ ë³€í™˜ë¨)
tokenized_dataset = tokenized_dataset.remove_columns(["review"])

# PyTorch Tensor í˜•ì‹ìœ¼ë¡œ ì„¸íŒ…
tokenized_dataset.set_format("torch")
print("âœ… í† í¬ë‚˜ì´ì§• ë° í…ì„œ ë³€í™˜ ì™„ë£Œ")

# 6. Train-Test Split (80% í›ˆë ¨, 20% í…ŒìŠ¤íŠ¸)

split_datasets = tokenized_dataset["train"].train_test_split(test_size=0.2)
tokenized_dataset = split_datasets  # ì—…ë°ì´íŠ¸

print("âœ… Train/Test ë¶„í•  ì™„ë£Œ")
print("ğŸ“Š Train ë°ì´í„° ê°œìˆ˜:", len(tokenized_dataset["train"]))
print("ğŸ“Š Test ë°ì´í„° ê°œìˆ˜:", len(tokenized_dataset["test"]))



# 7. KoBERT ëª¨ë¸ ë¡œë“œ (ë‹¨ì¼ ë¼ë²¨ ë¶„ë¥˜)
# ì˜ˆ: ë¼ë²¨ ë²”ìœ„ê°€ 0~6 ì´ 7ê°œë©´ num_labels=7
model = AutoModelForSequenceClassification.from_pretrained(
    "skt/kobert-base-v1",
    num_labels=7
)
print("âœ… KoBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# ë¼ë²¨ì´ float ë˜ëŠ” ë¬¸ìì—´ì´ë©´ ë³€í™˜
def cast_to_int(example):
    example["label"] = int(example["label"])  # í˜¹ì€ np.int64
    return example

tokenized_dataset = tokenized_dataset.map(cast_to_int)



import os
os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

# 8. Trainer ì„¤ì • (ë°°ì¹˜ í¬ê¸° ì¡°ì • ë° ìµœì í™”)
training_args = TrainingArguments(
    output_dir='./results',       # ì²´í¬í¬ì¸íŠ¸ ë° í•™ìŠµ ë¡œê·¸ ì €ì¥ í´ë”
    num_train_epochs=50,          # í•™ìŠµ Epoch ìˆ˜
    per_device_train_batch_size=4,  # âœ… GPU ë©”ëª¨ë¦¬ ìµœì í™” (256 â†’ 4ë¡œ ì¡°ì •)
    per_device_eval_batch_size=4,
    save_total_limit=2,           # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê°œìˆ˜ ì œí•œ (ìµœì‹  2ê°œë§Œ ìœ ì§€)
    eval_strategy="epoch",        # âœ… ë§¤ epochë§ˆë‹¤ ê²€ì¦ ìˆ˜í–‰
    logging_strategy="steps",     # âœ… Training Lossê°€ ë¡œê·¸ì— ì¶œë ¥ë˜ë„ë¡ ì„¤ì •
    logging_steps=10,             # âœ… 10 ìŠ¤í…ë§ˆë‹¤ Training Loss ì¶œë ¥
    save_strategy="epoch",        # âœ… Epoch ë‹¨ìœ„ë¡œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    logging_dir='./logs',         # ë¡œê·¸ ì €ì¥ ê²½ë¡œ
    save_steps=500,               # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë¹ˆë„
    report_to="none"              # âœ… W&B ë¹„í™œì„±í™” (í•„ìš”í•˜ë©´ "wandb"ë¡œ ë³€ê²½ ê°€ëŠ¥)
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"]
)

# ğŸš€ í•™ìŠµ ì‹œì‘
trainer.train()

# âœ… Training Loss ë¡œê·¸ í™•ì¸
for log in trainer.state.log_history:
    print(log)


# 9. ëª¨ë¸ í•™ìŠµ

print("ğŸš€ í•™ìŠµ ì‹œì‘!")
trainer.train()
print("ğŸš€ í•™ìŠµ ì™„ë£Œ!")


# 10. ìµœì¢… í‰ê°€ (Trainer ì‚¬ìš©)

print("ğŸ” í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€ ì‹œì‘...")
predictions = trainer.predict(tokenized_dataset["test"])

# ì˜ˆì¸¡ ë¡œì§“ì—ì„œ argmax
preds = torch.argmax(torch.tensor(predictions.predictions), dim=1)

# ì‹¤ì œ ë¼ë²¨
labels = tokenized_dataset["test"]["label"][:]

# ì •í™•ë„ ê³„ì‚°
acc = accuracy_score(labels, preds.tolist())
print(f"ğŸ¯ ìµœì¢… ì •í™•ë„: {acc:.4f}")

